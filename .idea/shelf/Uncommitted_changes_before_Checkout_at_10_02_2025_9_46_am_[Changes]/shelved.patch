Index: Configs/config_kanade_30_epochs.yml
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>log_dir: \"Models/DynamicBatching\"\r\n#first_stage_path: \"epoch_1st_00040.pth\" #This is the path within the models/log_dir, not the relative path from where it's being run. \r\nsave_freq: 10\r\nlog_interval: 10\r\ndevice: \"cuda\"\r\nepochs_1st: 50 # number of epochs for first stage training (pre-training)\r\nepochs_2nd: 30 # number of peochs for second stage training (joint training)\r\nprobe_batch: 50 # whether to automatically determine batch sizes, serves as max batch (set to None otherwise or when already calculated)\r\nsecond_stage_load_pretrained: false # set to true if the pre-trained model is for 2nd stage\r\nload_only_params: false # set to true if do not want to load epoch numbers and optimizer parameters\r\ndebug: false\r\ntracker: wandb\r\n\r\nF0_path: \"StyleTTS2Utils/JDC/bst.t7\"\r\nASR_config: \"StyleTTS2Utils/ASR/config.yml\"\r\nASR_path: \"StyleTTS2Utils/ASR/epoch_00080.pth\"\r\nPLBERT_dir: 'StyleTTS2Utils/PLBERT/'\r\n\r\n\r\n\r\ndata_params:\r\n  train_data: data_paths/train_data_filtered.txt\r\n  val_data: data_paths/val_data_filtered.txt\r\n  root_path: \"data_paths/\"\r\n  OOD_data: data_paths/ood_data_filtered.txt\r\n  min_length: 50 # sample until texts with this size are obtained for OOD texts\r\n\r\n\r\npreprocess_params:\r\n  sr: 24000\r\n  spect_params:\r\n    n_fft: 2048\r\n    win_length: 1200\r\n    hop_length: 300\r\n\r\nmodel_params:\r\n  multispeaker: true\r\n\r\n  dim_in: 64 \r\n  hidden_dim: 512\r\n  max_conv_dim: 512\r\n  n_layer: 3\r\n  n_mels: 80\r\n\r\n  n_token: 178 # number of phoneme tokens\r\n  max_dur: 50 # maximum duration of a single phoneme\r\n  style_dim: 128 # style vector size\r\n  \r\n  dropout: 0.2\r\n\r\n  decoder: \r\n    type: 'istftnet' # either hifigan or istftnet\r\n    resblock_kernel_sizes: [3,7,11]\r\n    upsample_rates :  [10, 6]\r\n    upsample_initial_channel: 512\r\n    resblock_dilation_sizes: [[1,3,5], [1,3,5], [1,3,5]]\r\n    upsample_kernel_sizes: [20, 12]\r\n    gen_istft_n_fft: 20\r\n    gen_istft_hop_size: 5\r\n\r\n\r\n      \r\n  # speech language model config\r\n  slm:\r\n      model: 'Respair/Whisper_Large_v2_Encoder_Block' # The model itself is hardcoded, change it through -> losses.py\r\n      sr: 16000 # sampling rate of SLM\r\n      hidden: 1280 # hidden size of SLM\r\n      nlayers: 33 # number of layers of SLM\r\n      initial_channel: 64 # initial channels of SLM discriminator head\r\n  \r\n  # style diffusion model config\r\n  diffusion:\r\n    embedding_mask_proba: 0.1\r\n    # transformer config\r\n    transformer:\r\n      num_layers: 3\r\n      num_heads: 8\r\n      head_features: 64\r\n      multiplier: 2\r\n\r\n    # diffusion distribution config\r\n    dist:\r\n      sigma_data: 0.2 # placeholder for estimate_sigma_data set to false\r\n      estimate_sigma_data: true # estimate sigma_data from the current batch if set to true\r\n      mean: -3.0\r\n      std: 1.0\r\n  \r\nloss_params:\r\n    lambda_mel: 10. # mel reconstruction loss\r\n    lambda_gen: 1. # generator loss\r\n    lambda_slm: 1. # slm feature matching loss\r\n    \r\n    lambda_mono: 1. # monotonic alignment loss (1st stage, TMA)\r\n    lambda_s2s: 1. # sequence-to-sequence loss (1st stage, TMA)\r\n    TMA_epoch: 5 # TMA starting epoch (1st stage)\r\n\r\n    lambda_F0: 1. # F0 reconstruction loss (2nd stage)\r\n    lambda_norm: 1. # norm reconstruction loss (2nd stage)\r\n    lambda_dur: 1. # duration loss (2nd stage)\r\n    lambda_ce: 20. # duration predictor probability output CE loss (2nd stage)\r\n    lambda_sty: 1. # style reconstruction loss (2nd stage)\r\n    lambda_diff: 1. # score matching loss (2nd stage)\r\n    \r\n    diff_epoch: 4 # style diffusion starting epoch (2nd stage)\r\n    joint_epoch: 999 # joint training starting epoch (2nd stage)\r\n\r\noptimizer_params:\r\n  lr: 0.00005 # general learning rate\r\n  bert_lr: 0.00001 # learning rate for PLBERT\r\n  ft_lr: 0.00001 # learning rate for acoustic modules\r\n  \r\nslmadv_params:\r\n  min_len: 400 # minimum length of samples\r\n  max_len: 500 # maximum length of samples\r\n  batch_percentage: 0.5 # to prevent out of memory, only use half of the original batch size\r\n  iter: 20 # update the discriminator every this iterations of generator update\r\n  thresh: 5 # gradient norm above which the gradient is scaled\r\n  scale: 0.01 # gradient scaling factor for predictors from SLM discriminators\r\n  sig: 1.5 # sigma for differentiable duration modeling\r\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/Configs/config_kanade_30_epochs.yml b/Configs/config_kanade_30_epochs.yml
--- a/Configs/config_kanade_30_epochs.yml	(revision a1e0f9ef5809eb8326a4dc6bbdfd59ac8ead4993)
+++ b/Configs/config_kanade_30_epochs.yml	(date 1738908509418)
@@ -29,6 +29,7 @@
 preprocess_params:
   sr: 24000
   spect_params:
+    
     n_fft: 2048
     win_length: 1200
     hop_length: 300
